{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from datasets import Dataset, load_dataset\n",
    "import librosa\n",
    "from keras import layers, models\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Numpy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"google/speech_commands\", \"v0.01\")\n",
    "# Overview of the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check available devices\n",
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_audio(set):\n",
    "\n",
    "    audio_array = set['audio']['array']\n",
    "    sampling_rate = set['audio']['sampling_rate']\n",
    "\n",
    "    mel_spectogram = librosa.feature.melspectrogram(y=audio_array, sr=sampling_rate, n_mels=128)\n",
    "\n",
    "    log_mel_spectogram = librosa.power_to_db(mel_spectogram)\n",
    "\n",
    "    log_mel_spectogram = np.expand_dims(log_mel_spectogram, axis=-1)\n",
    "\n",
    "    return {'audio': log_mel_spectogram}\n",
    "\n",
    "train_dataset = dataset['train'].map(preprocess_audio)\n",
    "validation_dataset = dataset['validation'].map(preprocess_audio)\n",
    "test_dataset = dataset['test'].map(preprocess_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# A simple CNN model\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(shape=(128, 32, 1)),\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(12, activation='softmax') # Number of possible commands\n",
    "])\n",
    "\n",
    "# Compiling\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def audio_generator(dataset):\n",
    "    for sample in dataset:\n",
    "        audio_features = sample['audio']\n",
    "        label = sample['label']\n",
    "        \n",
    "        # Convert audio_features to a numpy array (if it's not already)\n",
    "        audio_features = np.array(audio_features)\n",
    "        \n",
    "        # Ensure the audio features have the shape (128, 32, 1)\n",
    "        # Pad or truncate if necessary (this assumes the audio data is 2D, with shape (128, n_features, 1))\n",
    "        if audio_features.shape[1] < 32:\n",
    "            # Pad the sequence if it's shorter than expected\n",
    "            pad_width = 32 - audio_features.shape[1]\n",
    "            audio_features = np.pad(audio_features, ((0, 0), (0, pad_width), (0, 0)), mode='constant')\n",
    "        elif audio_features.shape[1] > 32:\n",
    "            # Truncate the sequence if it's longer than expected\n",
    "            audio_features = audio_features[:, :32, :]\n",
    "        \n",
    "        # Yield the audio features and label\n",
    "        yield audio_features, label\n",
    "\n",
    "def convert_to_tf_dataset(dataset):\n",
    "    # Create a TensorFlow Dataset from the generator\n",
    "    tf_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: audio_generator(dataset), \n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(128, 32, 1), dtype=tf.float32),  # The expected shape of the audio data\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int64)  # Adjust dtype according to your label type (e.g., tf.int64 for class labels)\n",
    "        )\n",
    "    )\n",
    "    return tf_dataset\n",
    "\n",
    "\n",
    "# Convert the train, validation, and test datasets\n",
    "train_tf_dataset = convert_to_tf_dataset(train_dataset)\n",
    "validation_tf_dataset = convert_to_tf_dataset(validation_dataset)\n",
    "test_tf_dataset = convert_to_tf_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Fitting the model now!!!\")\n",
    "\n",
    "model.fit(\n",
    "    train_tf_dataset.batch(128),\n",
    "    epochs=10,  # Can be changed\n",
    "    validation_data=validation_tf_dataset.batch(128)\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_tf_dataset)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
